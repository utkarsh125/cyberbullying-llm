# Cyberbullying Detection using LLM

<p align="center">
  <img src="https://github.com/user-attachments/assets/aedc5074-1b31-450c-8c0c-d854f613e442" align="center" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/LLM-Gemini--2.0--Flash-blue?style=for-the-badge&logo=google" />
  <img src="https://img.shields.io/badge/TypeScript-Enabled-3178c6?style=for-the-badge&logo=typescript&logoColor=white" />
  <img src="https://img.shields.io/badge/TailwindCSS-Styled-38bdf8?style=for-the-badge&logo=tailwind-css&logoColor=white" />
</p>

---

Since the introduction of LLM models via API inference, I believe it's pointless to reinvent the wheel by building profanity or racism detectors from scratch. It’s too much work when you can offload everything to a **pretrained LLM** that is both accurate and cost-effective.

This project utilizes **Gemini-2.0-Flash** and was built entirely with **TypeScript** and **Tailwind CSS** — nothing more, nothing less.

Just a simple, clean semester project that gets the job done.
